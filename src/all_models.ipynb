{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rb58853/embeddings-chatbot-testing/blob/main/src/all_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intall libraries and git clone"
      ],
      "metadata": {
        "id": "n4-xHZCGArAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jRh9AoLBAlyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e1a464-fc33-4168-9e28-85319deccb10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'embeddings'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 21 (delta 4), reused 18 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (21/21), done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "#Clone repo\n",
        "!git clone https://github.com/ML-Research-Team/embeddings.git\n",
        "\n",
        "import os\n",
        "os.chdir('embeddings/src')\n",
        "\n",
        "#Install libraries\n",
        "!pip install 'transformers[torch]'\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Models"
      ],
      "metadata": {
        "id": "Aa36Qjo-A7bR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imkuNJPLAlyv"
      },
      "outputs": [],
      "source": [
        "from utils import EmbeddingModel, similarity_by_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple tsting for colors with rgb vector"
      ],
      "metadata": {
        "id": "IXx0iidrBBZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDu5-zXsAlyv"
      },
      "outputs": [],
      "source": [
        "aquamarine = [20, 230, 160]\n",
        "olive = [120, 230, 120]\n",
        "red = [222, 15, 25]\n",
        "yellow = [255, 255, 0]\n",
        "orange = [255, 165, 0]\n",
        "blue = [40,40, 255]\n",
        "green = [45,244,22]\n",
        "blue_green = [0, 128, 128]\n",
        "\n",
        "v1 = blue_green\n",
        "v2 = aquamarine\n",
        "\n",
        "print(f'vector: {similarity_by_vector(v1, v2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing all models with simples examples"
      ],
      "metadata": {
        "id": "t-eRFsNoB290"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e5_small =EmbeddingModel(\"intfloat/e5-small\")\n",
        "e5_large =EmbeddingModel(\"intfloat/e5-large\")\n",
        "multilingual_e5_small = EmbeddingModel(\"intfloat/multilingual-e5-small\")\n",
        "distilbert_base_uncased = EmbeddingModel(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "ppy9lKwRCFLP"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [e5_small, e5_large, multilingual_e5_small,distilbert_base_uncased]"
      ],
      "metadata": {
        "id": "E9XlU-Bt2Cfy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_text_vs_all(texts, models):\n",
        "  for model in models:\n",
        "    print(f'{model.model_name}:\\n')\n",
        "    text1 = texts[0]\n",
        "    for j in range(1,len(texts)):\n",
        "        text2 = texts[j]\n",
        "        print(f'{text1} vs {text2}:\\n - {model.similarity(text1,text2)}')\n",
        "    print('__________________________________________________________________________')"
      ],
      "metadata": {
        "id": "E4yHZ_Ynu0r0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"raqueta de padel para zurdos\",\n",
        "    \"es bolsa de raquetas de padel para zurdos\",\n",
        "    \"es raqueta de padel para ambidietros\",\n",
        "    ]\n",
        "first_text_vs_all(texts, models)\n",
        "\n",
        "print('\\nkeywords')\n",
        "texts = [\n",
        "    \"raqueta padel zurdos\",\n",
        "    \"bolsa raqueta padel zurdos\",\n",
        "    \"raqueta padel ambidiestros\",\n",
        "]\n",
        "first_text_vs_all(texts, models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_HuSoLKk1_-",
        "outputId": "0ccf956d-0f7c-4bd0-e4a4-ddcbcaf4fe5c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "intfloat/e5-small:\n",
            "\n",
            "raqueta de padel para zurdos vs es bolsa de raquetas de padel para zurdos:\n",
            " - 0.9600124359130859\n",
            "raqueta de padel para zurdos vs es raqueta de padel para ambidietros:\n",
            " - 0.9360686540603638\n",
            "__________________________________________________________________________\n",
            "intfloat/e5-large:\n",
            "\n",
            "raqueta de padel para zurdos vs es bolsa de raquetas de padel para zurdos:\n",
            " - 0.9597803354263306\n",
            "raqueta de padel para zurdos vs es raqueta de padel para ambidietros:\n",
            " - 0.9393530488014221\n",
            "__________________________________________________________________________\n",
            "intfloat/multilingual-e5-small:\n",
            "\n",
            "raqueta de padel para zurdos vs es bolsa de raquetas de padel para zurdos:\n",
            " - 0.9593085050582886\n",
            "raqueta de padel para zurdos vs es raqueta de padel para ambidietros:\n",
            " - 0.9421345591545105\n",
            "__________________________________________________________________________\n",
            "distilbert-base-uncased:\n",
            "\n",
            "raqueta de padel para zurdos vs es bolsa de raquetas de padel para zurdos:\n",
            " - 0.9687214493751526\n",
            "raqueta de padel para zurdos vs es raqueta de padel para ambidietros:\n",
            " - 0.9608324766159058\n",
            "__________________________________________________________________________\n",
            "\n",
            "keywords\n",
            "intfloat/e5-small:\n",
            "\n",
            "raqueta padel zurdos vs bolsa raqueta padel zurdos:\n",
            " - 0.9728636741638184\n",
            "raqueta padel zurdos vs raqueta padel ambidiestros:\n",
            " - 0.9376614093780518\n",
            "__________________________________________________________________________\n",
            "intfloat/e5-large:\n",
            "\n",
            "raqueta padel zurdos vs bolsa raqueta padel zurdos:\n",
            " - 0.9485098123550415\n",
            "raqueta padel zurdos vs raqueta padel ambidiestros:\n",
            " - 0.929451048374176\n",
            "__________________________________________________________________________\n",
            "intfloat/multilingual-e5-small:\n",
            "\n",
            "raqueta padel zurdos vs bolsa raqueta padel zurdos:\n",
            " - 0.979324996471405\n",
            "raqueta padel zurdos vs raqueta padel ambidiestros:\n",
            " - 0.9461262822151184\n",
            "__________________________________________________________________________\n",
            "distilbert-base-uncased:\n",
            "\n",
            "raqueta padel zurdos vs bolsa raqueta padel zurdos:\n",
            " - 0.9760878086090088\n",
            "raqueta padel zurdos vs raqueta padel ambidiestros:\n",
            " - 0.9561627507209778\n",
            "__________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"Pantalon talla extra large\",\n",
        "    \"Pantalon talla extra small\",\n",
        "    \"Pantalon talla medium\",\n",
        "    ]\n",
        "first_text_vs_all(texts, models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5CFAPDjmlGj",
        "outputId": "bf66bfb8-cedb-4561-eeea-d75eb431970e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "intfloat/e5-small:\n",
            "\n",
            "Pantalon talla extra large vs Pantalon talla extra small:\n",
            " - 0.9696727991104126\n",
            "Pantalon talla extra large vs Pantalon talla medium:\n",
            " - 0.9482710957527161\n",
            "__________________________________________________________________________\n",
            "intfloat/e5-large:\n",
            "\n",
            "Pantalon talla extra large vs Pantalon talla extra small:\n",
            " - 0.9559967517852783\n",
            "Pantalon talla extra large vs Pantalon talla medium:\n",
            " - 0.9372074604034424\n",
            "__________________________________________________________________________\n",
            "intfloat/multilingual-e5-small:\n",
            "\n",
            "Pantalon talla extra large vs Pantalon talla extra small:\n",
            " - 0.9658886194229126\n",
            "Pantalon talla extra large vs Pantalon talla medium:\n",
            " - 0.9177154898643494\n",
            "__________________________________________________________________________\n",
            "distilbert-base-uncased:\n",
            "\n",
            "Pantalon talla extra large vs Pantalon talla extra small:\n",
            " - 0.9814617037773132\n",
            "Pantalon talla extra large vs Pantalon talla medium:\n",
            " - 0.8940823078155518\n",
            "__________________________________________________________________________\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IXx0iidrBBZp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}